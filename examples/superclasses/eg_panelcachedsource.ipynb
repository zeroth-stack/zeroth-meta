{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file we will explore how a CachedSource class works. Specifically, we will take the example of a Su, which outputs a DataFrame which has both time-series and cross-sectional dimensions. Focus on how the class automatically caches data generated during previous calls to the instance of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by importing all the necessay libraries. You may have to change the system path here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zpmeta.superclasses.source import Su\n",
    "from zpmeta.metaclasses.singletons import MultitonMeta\n",
    "from pandas import DataFrame, Series, concat, MultiIndex, date_range, IndexSlice\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create a subclass of Su that generates a dataframe of random numbers. All we have to do is to implement the \"execute\" method of the superclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSu(Su, metaclass=MultitonMeta):\n",
    "    '''Subclasses Su to create a dataframe of random numbers.\n",
    "    Accepts a dictionary of parameters, including:\n",
    "    cols: list of column names\n",
    "    '''\n",
    "    def __init__(self, params: dict = None):\n",
    "        super(RandomSu, self).__init__(params)\n",
    "        self.appendable = dict(xs=True, ts=True)\n",
    "    \n",
    "    def _execute(self, call_type=None, entities=None, period=None):\n",
    "        cols = MultiIndex.from_product([val for val in entities.values()], names=entities.keys())\n",
    "        idx = date_range(period[0], period[1], freq=self.params['freq'])\n",
    "        result = DataFrame(np.random.randn(len(idx), len(cols)), columns=cols, index=idx)\n",
    "        \n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us insantiate it. Notice how we can set the frequency of data generated in the params while instantiating the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:args: ({'freq': 'B'},) ; kwds: {}\n",
      "INFO:root:Multiton checking registry for key: (<class '__main__.RandomSu'>, '{\"freq\": \"B\"}')\n",
      "INFO:root:Multiton No Instance of <class '__main__.RandomSu'> {\"freq\": \"B\"}\n",
      "INFO:root:Multiton Registering Instance of <class '__main__.RandomSu'> {\"freq\": \"B\"}\n"
     ]
    }
   ],
   "source": [
    "daily_df_source = RandomSu(dict(freq='B'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once instantiated, the instance of this class behaves like a function. A function that has \"memory\". This is a more sophisticated form of memoization.\n",
    "\n",
    "Let us call this function object to create some initial dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:RUN RandomSu {'freq': 'B'}\n",
      "INFO:root:RUN INITIAL: [{'Type': ['A', 'B', 'C'], 'ID': [1, 2]}] 2019-01-12 00:00:00 - 2019-01-31 00:00:00\n",
      "INFO:root:EXEC INITIAL: [{'Type': ['A', 'B', 'C'], 'ID': [1, 2]}] 2019-01-12 00:00:00 - 2019-01-31 00:00:00\n",
      "INFO:root:DONE RandomSu {'freq': 'B'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type               A                   B                   C          \n",
      "ID                 1         2         1         2         1         2\n",
      "2019-01-14 -0.934348  1.348178 -1.008590 -1.009998  0.407684  0.569892\n",
      "2019-01-15 -1.093173 -2.068498  0.902066 -0.057532  1.086023  1.346712\n",
      "2019-01-16 -1.254036  0.152006  0.167813 -1.148591  0.282965 -0.556388\n",
      "2019-01-17 -0.219429  0.431363  0.059553 -0.415651  1.046993 -0.282705\n",
      "2019-01-18 -0.120632 -0.155869  0.141650  0.238249  0.609768 -1.437770\n",
      "2019-01-21  1.288646 -0.151641 -1.356019 -1.797361  0.406475 -1.547839\n",
      "2019-01-22 -0.364274  0.028487 -0.622089 -1.641916  0.169292 -1.176435\n",
      "2019-01-23 -0.162016  1.088112  1.389108 -1.173184 -0.291912  1.619836\n",
      "2019-01-24  0.079664 -0.061402 -1.677258 -1.095664  1.346008 -1.207647\n",
      "2019-01-25  0.482326  0.086248  0.812211  0.265602  0.030575 -1.425877\n",
      "2019-01-28 -1.219299  1.839446  0.157506 -0.617554 -0.388732  1.980366\n",
      "2019-01-29 -0.964681 -1.782021 -1.168522 -1.819608 -0.056750  1.283246\n",
      "2019-01-30  0.274432  1.400724  0.387516  1.092441  0.575205  1.185397\n",
      "2019-01-31  0.314736  1.212551 -0.460984 -0.123553  1.194596 -0.154426\n"
     ]
    }
   ],
   "source": [
    "df = daily_df_source(entities=dict(Type=['A','B','C'], ID=[1,2]), period=(datetime(2019,1,12), datetime(2019,1,31)))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us give it some incremental columns. Notice how the class automatically recognizes the additional columns given and generates data only for that additional column and appends it to the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:RUN RandomSu {'freq': 'B'}\n",
      "INFO:root:RUN Nth: {'Type': ['C', 'D'], 'ID': [1, 2]} 2019-01-12 00:00:00 - 2019-01-31 00:00:00\n",
      "INFO:root:INCREMENTAL Items: {'Type': ['D'], 'ID': [1, 2]}\n",
      "INFO:root:TOTAL Items: {'Type': ['B', 'A', 'D', 'C'], 'ID': [1, 2]}\n",
      "INFO:root:DECREMENTAL Items: {}\n",
      "INFO:root:INCREMENTAL Period: None - None\n",
      "INFO:root:TOTAL Period: 2019-01-12 00:00:00 - 2019-01-31 00:00:00\n",
      "INFO:root:APPENDABLE XS:True TS:True\n",
      "INFO:root:EXEC INCREMENTAL XS1: [{'Type': ['D'], 'ID': [1, 2]}] 2019-01-12 00:00:00 - 2019-01-31 00:00:00\n",
      "INFO:root:DONE RandomSu {'freq': 'B'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type               A                   B                   C            \\\n",
      "ID                 1         2         1         2         1         2   \n",
      "2019-01-14  1.844280  0.020412  1.246756  1.154022  0.734224  0.398374   \n",
      "2019-01-15 -0.114657 -0.637401 -0.849754 -0.945679  0.153935 -0.035244   \n",
      "2019-01-16  0.655751  0.201677 -0.830324 -0.337273  0.793627  1.544500   \n",
      "2019-01-17  0.020507 -0.791126 -2.276656 -0.839476  0.066704 -1.295300   \n",
      "2019-01-18 -1.641192 -1.924791  0.368263 -0.100437  1.495483  0.126473   \n",
      "2019-01-21 -0.569048 -1.294371 -0.860440  2.993533  0.947108  1.632600   \n",
      "2019-01-22 -0.038089  1.513185 -0.966126  0.839116  0.114526  1.954067   \n",
      "2019-01-23  0.979712 -0.934915 -1.301877 -0.384024 -0.338961 -0.865196   \n",
      "2019-01-24  0.780730 -0.231587  1.679492 -0.675422 -0.951835  1.861684   \n",
      "2019-01-25  1.298808 -0.207203  1.145511  1.011837  0.501611  0.685172   \n",
      "2019-01-28  0.392768  0.316277 -0.845618 -1.210021 -0.932847  0.594189   \n",
      "2019-01-29 -0.917573 -0.614059  1.117753  1.130523  0.071147  0.584808   \n",
      "2019-01-30  2.251211  0.874264  0.486063  0.001212  1.073151 -0.975887   \n",
      "2019-01-31  0.799373 -1.830975 -1.035172 -0.086361  0.115759 -0.507227   \n",
      "\n",
      "Type               D            \n",
      "ID                 1         2  \n",
      "2019-01-14  1.130491 -0.673127  \n",
      "2019-01-15  0.508699 -0.940724  \n",
      "2019-01-16 -0.807375 -0.410450  \n",
      "2019-01-17  0.097735 -0.248652  \n",
      "2019-01-18  1.857078  0.360883  \n",
      "2019-01-21  0.476164  2.518687  \n",
      "2019-01-22  0.996834 -0.676677  \n",
      "2019-01-23  0.857541  0.641442  \n",
      "2019-01-24  1.158136  0.485029  \n",
      "2019-01-25  0.443740  1.476192  \n",
      "2019-01-28  0.254751 -0.807423  \n",
      "2019-01-29 -0.114017  0.709756  \n",
      "2019-01-30 -0.788694 -0.143493  \n",
      "2019-01-31  1.550818  1.485415  \n"
     ]
    }
   ],
   "source": [
    "df_xs_incremental = daily_df_source(entities=dict(Type=['C', 'D'], ID=[1,2]), period=(datetime(2019,1,12), datetime(2019,1,31)))\n",
    "print(df_xs_incremental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we give it the same set of columns but additional time period. Now it generates data only for the \"incremental\" period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:RUN RandomSu {'freq': 'B'}\n",
      "INFO:root:RUN Nth: {'Type': ['A', 'B', 'C', 'D'], 'ID': [1, 2]} 2019-01-20 00:00:00 - 2019-02-05 00:00:00\n",
      "INFO:root:INCREMENTAL Items: None\n",
      "INFO:root:TOTAL Items: {'Type': ['B', 'D', 'A', 'C'], 'ID': [1, 2]}\n",
      "INFO:root:DECREMENTAL Items: None\n",
      "INFO:root:INCREMENTAL Period: 2019-01-31 00:00:00 - 2019-02-05 00:00:00\n",
      "INFO:root:TOTAL Period: 2019-01-12 00:00:00 - 2019-02-05 00:00:00\n",
      "INFO:root:APPENDABLE XS:True TS:True\n",
      "INFO:root:EXEC INCREMENTAL TS1: [{'Type': ['B', 'A', 'D', 'C'], 'ID': [1, 2]}] 2019-01-31 00:00:00 - 2019-02-05 00:00:00\n",
      "INFO:root:DONE RandomSu {'freq': 'B'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type               A                   B                   C            \\\n",
      "ID                 1         2         1         2         1         2   \n",
      "2019-01-14  1.844280  0.020412  1.246756  1.154022  0.734224  0.398374   \n",
      "2019-01-15 -0.114657 -0.637401 -0.849754 -0.945679  0.153935 -0.035244   \n",
      "2019-01-16  0.655751  0.201677 -0.830324 -0.337273  0.793627  1.544500   \n",
      "2019-01-17  0.020507 -0.791126 -2.276656 -0.839476  0.066704 -1.295300   \n",
      "2019-01-18 -1.641192 -1.924791  0.368263 -0.100437  1.495483  0.126473   \n",
      "2019-01-21 -0.569048 -1.294371 -0.860440  2.993533  0.947108  1.632600   \n",
      "2019-01-22 -0.038089  1.513185 -0.966126  0.839116  0.114526  1.954067   \n",
      "2019-01-23  0.979712 -0.934915 -1.301877 -0.384024 -0.338961 -0.865196   \n",
      "2019-01-24  0.780730 -0.231587  1.679492 -0.675422 -0.951835  1.861684   \n",
      "2019-01-25  1.298808 -0.207203  1.145511  1.011837  0.501611  0.685172   \n",
      "2019-01-28  0.392768  0.316277 -0.845618 -1.210021 -0.932847  0.594189   \n",
      "2019-01-29 -0.917573 -0.614059  1.117753  1.130523  0.071147  0.584808   \n",
      "2019-01-30  2.251211  0.874264  0.486063  0.001212  1.073151 -0.975887   \n",
      "2019-01-31  0.799373 -1.830975 -1.035172 -0.086361  0.115759 -0.507227   \n",
      "2019-02-01  0.764245 -0.639821 -1.662154 -0.981748 -1.024188  2.271811   \n",
      "2019-02-04  1.266934 -0.674867 -1.271197  0.668955 -1.423638  1.863062   \n",
      "2019-02-05 -1.116971 -0.744488  0.285672 -0.737285 -0.849401  1.296432   \n",
      "\n",
      "Type               D            \n",
      "ID                 1         2  \n",
      "2019-01-14  1.130491 -0.673127  \n",
      "2019-01-15  0.508699 -0.940724  \n",
      "2019-01-16 -0.807375 -0.410450  \n",
      "2019-01-17  0.097735 -0.248652  \n",
      "2019-01-18  1.857078  0.360883  \n",
      "2019-01-21  0.476164  2.518687  \n",
      "2019-01-22  0.996834 -0.676677  \n",
      "2019-01-23  0.857541  0.641442  \n",
      "2019-01-24  1.158136  0.485029  \n",
      "2019-01-25  0.443740  1.476192  \n",
      "2019-01-28  0.254751 -0.807423  \n",
      "2019-01-29 -0.114017  0.709756  \n",
      "2019-01-30 -0.788694 -0.143493  \n",
      "2019-01-31  1.550818  1.485415  \n",
      "2019-02-01  0.710131  0.294676  \n",
      "2019-02-04  0.403095  1.276274  \n",
      "2019-02-05  0.666069  0.319279  \n"
     ]
    }
   ],
   "source": [
    "df_ts_incremental = daily_df_source(entities=dict(Type=['A','B','C','D'],ID=[1,2]), period=(datetime(2019,1,20), datetime(2019,2,5)))\n",
    "print(df_ts_incremental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us give it an example where we feed it both additional columns and additional period. As we can see, it will generate data first for only the incremental columns for the existing period, and then incremental dates for all the columns. This helps minimze calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:RUN RandomSu {'freq': 'B'}\n",
      "INFO:root:RUN Nth: {'Type': ['A', 'B', 'C', 'D', 'E'], 'ID': [1, 2]} 2019-01-20 00:00:00 - 2019-02-10 00:00:00\n",
      "INFO:root:INCREMENTAL Items: {'Type': ['E'], 'ID': [1, 2]}\n",
      "INFO:root:TOTAL Items: {'Type': ['B', 'D', 'E', 'A', 'C'], 'ID': [1, 2]}\n",
      "INFO:root:DECREMENTAL Items: None\n",
      "INFO:root:INCREMENTAL Period: 2019-02-05 00:00:00 - 2019-02-10 00:00:00\n",
      "INFO:root:TOTAL Period: 2019-01-12 00:00:00 - 2019-02-10 00:00:00\n",
      "INFO:root:APPENDABLE XS:True TS:True\n",
      "INFO:root:EXEC INCREMENTAL XS1: [{'Type': ['E'], 'ID': [1, 2]}] 2019-01-12 00:00:00 - 2019-02-05 00:00:00\n",
      "INFO:root:EXEC INCREMENTAL TS1: [{'Type': ['B', 'D', 'E', 'A', 'C'], 'ID': [1, 2]}] 2019-02-05 00:00:00 - 2019-02-10 00:00:00\n",
      "INFO:root:DONE RandomSu {'freq': 'B'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type               A                   B                   C            \\\n",
      "ID                 1         2         1         2         1         2   \n",
      "2019-01-14  1.844280  0.020412  1.246756  1.154022  0.734224  0.398374   \n",
      "2019-01-15 -0.114657 -0.637401 -0.849754 -0.945679  0.153935 -0.035244   \n",
      "2019-01-16  0.655751  0.201677 -0.830324 -0.337273  0.793627  1.544500   \n",
      "2019-01-17  0.020507 -0.791126 -2.276656 -0.839476  0.066704 -1.295300   \n",
      "2019-01-18 -1.641192 -1.924791  0.368263 -0.100437  1.495483  0.126473   \n",
      "2019-01-21 -0.569048 -1.294371 -0.860440  2.993533  0.947108  1.632600   \n",
      "2019-01-22 -0.038089  1.513185 -0.966126  0.839116  0.114526  1.954067   \n",
      "2019-01-23  0.979712 -0.934915 -1.301877 -0.384024 -0.338961 -0.865196   \n",
      "2019-01-24  0.780730 -0.231587  1.679492 -0.675422 -0.951835  1.861684   \n",
      "2019-01-25  1.298808 -0.207203  1.145511  1.011837  0.501611  0.685172   \n",
      "2019-01-28  0.392768  0.316277 -0.845618 -1.210021 -0.932847  0.594189   \n",
      "2019-01-29 -0.917573 -0.614059  1.117753  1.130523  0.071147  0.584808   \n",
      "2019-01-30  2.251211  0.874264  0.486063  0.001212  1.073151 -0.975887   \n",
      "2019-01-31  0.799373 -1.830975 -1.035172 -0.086361  0.115759 -0.507227   \n",
      "2019-02-01  0.764245 -0.639821 -1.662154 -0.981748 -1.024188  2.271811   \n",
      "2019-02-04  1.266934 -0.674867 -1.271197  0.668955 -1.423638  1.863062   \n",
      "2019-02-05 -1.116971 -0.744488  0.285672 -0.737285 -0.849401  1.296432   \n",
      "2019-02-06 -0.124417 -2.233606 -1.543296 -0.148242 -0.834467  0.052686   \n",
      "2019-02-07  0.733269  0.021125 -1.241125 -0.417982 -0.733344  0.464619   \n",
      "2019-02-08 -0.500446 -0.840877  0.228943 -0.080948 -0.727652 -0.197154   \n",
      "\n",
      "Type               D                   E            \n",
      "ID                 1         2         1         2  \n",
      "2019-01-14  1.130491 -0.673127  0.530251 -0.555589  \n",
      "2019-01-15  0.508699 -0.940724 -0.839013 -0.348913  \n",
      "2019-01-16 -0.807375 -0.410450 -0.144122 -1.186199  \n",
      "2019-01-17  0.097735 -0.248652  1.717425  0.273836  \n",
      "2019-01-18  1.857078  0.360883  0.128895  1.730166  \n",
      "2019-01-21  0.476164  2.518687 -0.969519  1.494941  \n",
      "2019-01-22  0.996834 -0.676677 -0.619965  0.903987  \n",
      "2019-01-23  0.857541  0.641442 -0.142547 -0.854606  \n",
      "2019-01-24  1.158136  0.485029  1.002974 -1.520761  \n",
      "2019-01-25  0.443740  1.476192  1.215614 -2.338752  \n",
      "2019-01-28  0.254751 -0.807423 -1.091002 -0.038643  \n",
      "2019-01-29 -0.114017  0.709756  0.102249  1.020652  \n",
      "2019-01-30 -0.788694 -0.143493 -1.138953 -0.382195  \n",
      "2019-01-31  1.550818  1.485415  0.033418 -0.552670  \n",
      "2019-02-01  0.710131  0.294676 -1.441783  0.619176  \n",
      "2019-02-04  0.403095  1.276274  0.740818 -0.186703  \n",
      "2019-02-05  0.666069  0.319279  0.800043  2.229543  \n",
      "2019-02-06 -0.906428 -2.082231 -0.072376  0.101606  \n",
      "2019-02-07  0.662336  0.552276  1.336085  0.024739  \n",
      "2019-02-08  0.998526 -0.436582  1.588101 -0.459663  \n"
     ]
    }
   ],
   "source": [
    "df_xsts_incremental = daily_df_source(entities=dict(Type=['A','B','C','D','E'], ID=[1,2]), period=(datetime(2019,1,20), datetime(2019,2,10)))\n",
    "print(df_xsts_incremental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, let us ask it for data which is a subset of previously generated data - no incremental columns or dates. It should not execute for any data, it will just use the prior generate data to returnt the correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:RUN RandomSu {'freq': 'B'}\n",
      "INFO:root:RUN Nth: {'Type': ['A', 'B', 'C', 'E'], 'ID': [1, 2]} 2019-01-20 00:00:00 - 2019-02-01 00:00:00\n",
      "INFO:root:INCREMENTAL Items: None\n",
      "INFO:root:TOTAL Items: {'Type': ['B', 'D', 'E', 'A', 'C'], 'ID': [1, 2]}\n",
      "INFO:root:DECREMENTAL Items: {}\n",
      "INFO:root:INCREMENTAL Period: None - None\n",
      "INFO:root:TOTAL Period: 2019-01-12 00:00:00 - 2019-02-10 00:00:00\n",
      "INFO:root:APPENDABLE XS:True TS:True\n",
      "INFO:root:DONE RandomSu {'freq': 'B'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type               A                   B                   C            \\\n",
      "ID                 1         2         1         2         1         2   \n",
      "2019-01-14  1.844280  0.020412  1.246756  1.154022  0.734224  0.398374   \n",
      "2019-01-15 -0.114657 -0.637401 -0.849754 -0.945679  0.153935 -0.035244   \n",
      "2019-01-16  0.655751  0.201677 -0.830324 -0.337273  0.793627  1.544500   \n",
      "2019-01-17  0.020507 -0.791126 -2.276656 -0.839476  0.066704 -1.295300   \n",
      "2019-01-18 -1.641192 -1.924791  0.368263 -0.100437  1.495483  0.126473   \n",
      "2019-01-21 -0.569048 -1.294371 -0.860440  2.993533  0.947108  1.632600   \n",
      "2019-01-22 -0.038089  1.513185 -0.966126  0.839116  0.114526  1.954067   \n",
      "2019-01-23  0.979712 -0.934915 -1.301877 -0.384024 -0.338961 -0.865196   \n",
      "2019-01-24  0.780730 -0.231587  1.679492 -0.675422 -0.951835  1.861684   \n",
      "2019-01-25  1.298808 -0.207203  1.145511  1.011837  0.501611  0.685172   \n",
      "2019-01-28  0.392768  0.316277 -0.845618 -1.210021 -0.932847  0.594189   \n",
      "2019-01-29 -0.917573 -0.614059  1.117753  1.130523  0.071147  0.584808   \n",
      "2019-01-30  2.251211  0.874264  0.486063  0.001212  1.073151 -0.975887   \n",
      "2019-01-31  0.799373 -1.830975 -1.035172 -0.086361  0.115759 -0.507227   \n",
      "2019-02-01  0.764245 -0.639821 -1.662154 -0.981748 -1.024188  2.271811   \n",
      "2019-02-04  1.266934 -0.674867 -1.271197  0.668955 -1.423638  1.863062   \n",
      "2019-02-05 -1.116971 -0.744488  0.285672 -0.737285 -0.849401  1.296432   \n",
      "2019-02-06 -0.124417 -2.233606 -1.543296 -0.148242 -0.834467  0.052686   \n",
      "2019-02-07  0.733269  0.021125 -1.241125 -0.417982 -0.733344  0.464619   \n",
      "2019-02-08 -0.500446 -0.840877  0.228943 -0.080948 -0.727652 -0.197154   \n",
      "\n",
      "Type               D                   E            \n",
      "ID                 1         2         1         2  \n",
      "2019-01-14  1.130491 -0.673127  0.530251 -0.555589  \n",
      "2019-01-15  0.508699 -0.940724 -0.839013 -0.348913  \n",
      "2019-01-16 -0.807375 -0.410450 -0.144122 -1.186199  \n",
      "2019-01-17  0.097735 -0.248652  1.717425  0.273836  \n",
      "2019-01-18  1.857078  0.360883  0.128895  1.730166  \n",
      "2019-01-21  0.476164  2.518687 -0.969519  1.494941  \n",
      "2019-01-22  0.996834 -0.676677 -0.619965  0.903987  \n",
      "2019-01-23  0.857541  0.641442 -0.142547 -0.854606  \n",
      "2019-01-24  1.158136  0.485029  1.002974 -1.520761  \n",
      "2019-01-25  0.443740  1.476192  1.215614 -2.338752  \n",
      "2019-01-28  0.254751 -0.807423 -1.091002 -0.038643  \n",
      "2019-01-29 -0.114017  0.709756  0.102249  1.020652  \n",
      "2019-01-30 -0.788694 -0.143493 -1.138953 -0.382195  \n",
      "2019-01-31  1.550818  1.485415  0.033418 -0.552670  \n",
      "2019-02-01  0.710131  0.294676 -1.441783  0.619176  \n",
      "2019-02-04  0.403095  1.276274  0.740818 -0.186703  \n",
      "2019-02-05  0.666069  0.319279  0.800043  2.229543  \n",
      "2019-02-06 -0.906428 -2.082231 -0.072376  0.101606  \n",
      "2019-02-07  0.662336  0.552276  1.336085  0.024739  \n",
      "2019-02-08  0.998526 -0.436582  1.588101 -0.459663  \n"
     ]
    }
   ],
   "source": [
    "df_xsts_subset = daily_df_source(entities=dict(Type=['A','B','C','E'], ID=[1,2]), period=(datetime(2019,1,20), datetime(2019,2,1)))\n",
    "print(df_xsts_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When combined with the MultitonMeta metaclass, this becomes even more powerful, leading to significant efficiencies and resuability of data in a complex simuation. Examples of using the MultitonMeta metaclass follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first try to instantiate another object RandomSu with the same params. As can be seen here, it found the prior instance in the registry and returns us the same instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:args: () ; kwds: {'params': {'freq': 'B'}}\n",
      "INFO:root:Multiton checking registry for key: (<class '__main__.RandomSu'>, '{\"freq\": \"B\"}')\n",
      "INFO:root:Multiton Found Instance of <class '__main__.RandomSu'> {\"freq\": \"B\"}\n"
     ]
    }
   ],
   "source": [
    "daily_df_source_new = RandomSu(params=dict(freq='B'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prior instance already has the data in its cache, let us check for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:RUN RandomSu {'freq': 'B'}\n",
      "INFO:root:RUN Nth: {'Type': ['A', 'B', 'C', 'E'], 'ID': [1, 2]} 2019-01-20 00:00:00 - 2019-02-01 00:00:00\n",
      "INFO:root:INCREMENTAL Items: None\n",
      "INFO:root:TOTAL Items: {'Type': ['B', 'D', 'E', 'A', 'C'], 'ID': [1, 2]}\n",
      "INFO:root:DECREMENTAL Items: {}\n",
      "INFO:root:INCREMENTAL Period: None - None\n",
      "INFO:root:TOTAL Period: 2019-01-12 00:00:00 - 2019-02-10 00:00:00\n",
      "INFO:root:APPENDABLE XS:True TS:True\n",
      "INFO:root:DONE RandomSu {'freq': 'B'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type               A                   B                   C            \\\n",
      "ID                 1         2         1         2         1         2   \n",
      "2019-01-14  1.844280  0.020412  1.246756  1.154022  0.734224  0.398374   \n",
      "2019-01-15 -0.114657 -0.637401 -0.849754 -0.945679  0.153935 -0.035244   \n",
      "2019-01-16  0.655751  0.201677 -0.830324 -0.337273  0.793627  1.544500   \n",
      "2019-01-17  0.020507 -0.791126 -2.276656 -0.839476  0.066704 -1.295300   \n",
      "2019-01-18 -1.641192 -1.924791  0.368263 -0.100437  1.495483  0.126473   \n",
      "2019-01-21 -0.569048 -1.294371 -0.860440  2.993533  0.947108  1.632600   \n",
      "2019-01-22 -0.038089  1.513185 -0.966126  0.839116  0.114526  1.954067   \n",
      "2019-01-23  0.979712 -0.934915 -1.301877 -0.384024 -0.338961 -0.865196   \n",
      "2019-01-24  0.780730 -0.231587  1.679492 -0.675422 -0.951835  1.861684   \n",
      "2019-01-25  1.298808 -0.207203  1.145511  1.011837  0.501611  0.685172   \n",
      "2019-01-28  0.392768  0.316277 -0.845618 -1.210021 -0.932847  0.594189   \n",
      "2019-01-29 -0.917573 -0.614059  1.117753  1.130523  0.071147  0.584808   \n",
      "2019-01-30  2.251211  0.874264  0.486063  0.001212  1.073151 -0.975887   \n",
      "2019-01-31  0.799373 -1.830975 -1.035172 -0.086361  0.115759 -0.507227   \n",
      "2019-02-01  0.764245 -0.639821 -1.662154 -0.981748 -1.024188  2.271811   \n",
      "2019-02-04  1.266934 -0.674867 -1.271197  0.668955 -1.423638  1.863062   \n",
      "2019-02-05 -1.116971 -0.744488  0.285672 -0.737285 -0.849401  1.296432   \n",
      "2019-02-06 -0.124417 -2.233606 -1.543296 -0.148242 -0.834467  0.052686   \n",
      "2019-02-07  0.733269  0.021125 -1.241125 -0.417982 -0.733344  0.464619   \n",
      "2019-02-08 -0.500446 -0.840877  0.228943 -0.080948 -0.727652 -0.197154   \n",
      "\n",
      "Type               D                   E            \n",
      "ID                 1         2         1         2  \n",
      "2019-01-14  1.130491 -0.673127  0.530251 -0.555589  \n",
      "2019-01-15  0.508699 -0.940724 -0.839013 -0.348913  \n",
      "2019-01-16 -0.807375 -0.410450 -0.144122 -1.186199  \n",
      "2019-01-17  0.097735 -0.248652  1.717425  0.273836  \n",
      "2019-01-18  1.857078  0.360883  0.128895  1.730166  \n",
      "2019-01-21  0.476164  2.518687 -0.969519  1.494941  \n",
      "2019-01-22  0.996834 -0.676677 -0.619965  0.903987  \n",
      "2019-01-23  0.857541  0.641442 -0.142547 -0.854606  \n",
      "2019-01-24  1.158136  0.485029  1.002974 -1.520761  \n",
      "2019-01-25  0.443740  1.476192  1.215614 -2.338752  \n",
      "2019-01-28  0.254751 -0.807423 -1.091002 -0.038643  \n",
      "2019-01-29 -0.114017  0.709756  0.102249  1.020652  \n",
      "2019-01-30 -0.788694 -0.143493 -1.138953 -0.382195  \n",
      "2019-01-31  1.550818  1.485415  0.033418 -0.552670  \n",
      "2019-02-01  0.710131  0.294676 -1.441783  0.619176  \n",
      "2019-02-04  0.403095  1.276274  0.740818 -0.186703  \n",
      "2019-02-05  0.666069  0.319279  0.800043  2.229543  \n",
      "2019-02-06 -0.906428 -2.082231 -0.072376  0.101606  \n",
      "2019-02-07  0.662336  0.552276  1.336085  0.024739  \n",
      "2019-02-08  0.998526 -0.436582  1.588101 -0.459663  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "eval() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m df_xsts_subset_2 \u001b[39m=\u001b[39m daily_df_source_new(entities\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(Type\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mE\u001b[39m\u001b[39m'\u001b[39m], ID\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m]), period\u001b[39m=\u001b[39m(datetime(\u001b[39m2019\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m20\u001b[39m), datetime(\u001b[39m2019\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)))\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(df_xsts_subset_2)\n\u001b[1;32m----> 4\u001b[0m df_xsts_subset_2\u001b[39m.\u001b[39;49mquery(\u001b[39m'\u001b[39;49m\u001b[39mType == \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m and ID == 1\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4474\u001b[0m, in \u001b[0;36mDataFrame.query\u001b[1;34m(self, expr, inplace, **kwargs)\u001b[0m\n\u001b[0;32m   4472\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m   4473\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 4474\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval(expr, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   4476\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   4477\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc[res]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4612\u001b[0m, in \u001b[0;36mDataFrame.eval\u001b[1;34m(self, expr, inplace, **kwargs)\u001b[0m\n\u001b[0;32m   4609\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m   4610\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mresolvers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mresolvers\u001b[39m\u001b[39m\"\u001b[39m, ())) \u001b[39m+\u001b[39m resolvers\n\u001b[1;32m-> 4612\u001b[0m \u001b[39mreturn\u001b[39;00m _eval(expr, inplace\u001b[39m=\u001b[39minplace, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: eval() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "df_xsts_subset_2 = daily_df_source_new(entities=dict(Type=['A','B','C','E'], ID=[1,2]), period=(datetime(2019,1,20), datetime(2019,2,1)))\n",
    "print(df_xsts_subset_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No additional execution was necessary.\n",
    "\n",
    "Now let us create an instance of RandomSu but with a different set of params for annual data generation. It will not find the class in the registry and will create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:args: () ; kwds: {'params': {'freq': 'A'}}\n",
      "INFO:root:Multiton checking registry for key: (<class '__main__.RandomSu'>, '{\"freq\": \"A\"}')\n",
      "INFO:root:Multiton No Instance of <class '__main__.RandomSu'> {\"freq\": \"A\"}\n",
      "INFO:root:Multiton Registering Instance of <class '__main__.RandomSu'> {\"freq\": \"A\"}\n"
     ]
    }
   ],
   "source": [
    "annual_df_source = RandomSu(params=dict(freq='A'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, wherever in the code an annual RandomSu is instantiated, it will access the same instance which also has the data for all the prior calls saved in it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
